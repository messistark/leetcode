package cn.spark.study.core

import org.apache.spark.{SparkConf, SparkContext}

object GroupTop3 {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf()
      .setAppName("GroupTop3")
      .setMaster("local")
    val sc = new SparkContext(conf)

    val lines = sc.textFile("C://Users//gaochen//Desktop//score.txt")

    val line = lines.map(line => (line.split(" ")(0),
      line.split(" ")(1).toInt))
    val groups = line.groupByKey()
    val groupSort = groups.map(tu =>{
      val key = tu._1
      val value = tu._2
      val sortValues = value.toList.sortWith(_ > _).take(3)   // sortBy(_._3).reverse
      (key,sortValues)
    })
    groupSort.sortBy(tu => tu._1,false,1).foreach(x =>{
      println(x._1)
      x._2.foreach((v => println("\t"+ v)))
      println("================")
    })
    sc.stop()
  }
}
————————————————
版权声明：本文为CSDN博主「菜鸟也有梦想啊」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/huaicainiao/article/details/90243955



使用RDD中的sortBy方法(适用于数据量较大的情况)

val lines=sc.textFile(path)
 
val subjectAndTeacher=lines.map(func)
 
val subjects=subjectAndTeacher.keys.distinct()  //获取所有科目
 
val maped=subjectAndTeacher.map((_,1))
 
val reduced=maped.reduceByKey(_+_)
 
for(sb <- subjects){   
 
  val filter=reduce.filter(_._1.equals(sb))  //获取该学科的所有老师
 
  //现在调用的是RDD上的sortBy方法，可在内存与磁盘中
 
  //take方法是先在Executor中取好前几个再通过网络发送到Driver,是个      //Action
 
  val r=filter.sortBy(_._2,false).take(topN)   //rdd排序
 
  //然后把r收集或存储起来
 
}

————————————————
版权声明：本文为CSDN博主「一溪云~」的原创文章，遵循CC 4.0 BY-SA版权协议，转载请附上原文出处链接及本声明。
原文链接：https://blog.csdn.net/qq_34715484/article/details/83153490
